\documentclass[conference]{IEEEtran}
\usepackage{IEEEpreamble}

\usepackage{url}
\usepackage{booktabs}
\usepackage{csquotes}

\newcommand{\urlcount}{2,127,284}
\newcommand{\xlscount}{249,376}
\newcommand{\urlcountunique}{292,043}

\begin{document}


\title{A Quarter of Million Insights about End-User Programmers}

\author{
\IEEEauthorblockN{Kevin Lubick\IEEEauthorrefmark{1},
Titus Barik\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, 
Emerson Murphy-Hill\IEEEauthorrefmark{1}}
\IEEEauthorblockA{
\IEEEauthorrefmark{1}North Carolina State University, Raleigh, North Carolina, USA\\
\IEEEauthorrefmark{2}ABB Corporate Research, Raleigh, North Carolina, USA\\
 kjlubick@ncsu.edu, titus.barik@us.abb.com, emerson@csc.ncsu.edu
}}


\maketitle

\begin{abstract}
We are proposing the \textsc{Fuse} corpus to be used as the dataset for the MSR 2016 data challenge.
The \textsc{Fuse} corpus was presented at the MSR 2015 data showcase, containing \xlscount{} unique spreadsheets --- an order of magnitude larger than previous corpora.
In this paper we make a case for how this diverse corpus of spreadsheets will offer large benefits to the end-user and software engineering community.
We conclude with some promising lines of research \textsc{Fuse} allows and give an overview of the tools we provide to aid researchers who will use our dataset for the challenge.

\end{abstract}


\IEEEpeerreviewmaketitle

\section{Introduction}

End-user programmers today constitute a broad class of users, including teachers, accountants, administrators, managers, research scientists, and even children~\cite{Ko2011}.
Although these users are typically not professional software developers, their roles routinely involve computational tasks that, in many ways, are similar to those of developers --- not just in activity, but also in their underlying cognitive demands on users~\cite{Blackwell2002}. 

Perhaps the most ubiquitous form~\cite{Scaffidi2005} of end-user programming software are \emph{spreadsheets}, a table-oriented visual interface that serves as the underlying model for the users' applications~\cite{Nardi1990}. \emph{Cells} within these tables are augmented with computation, such as expressions, functions and macros~\cite{Nardi1990}. 
This interplay between presentation and computation within the spreadsheet environment has garnered significant interest from the software engineering research community~\cite{Burnett2009}. 
Researchers have adopted techniques and approaches to studying errors~\cite{Powell2008}, code smells~\cite{Pinzger2012}, and refactoring in spreadsheets~\cite{Badame2012}, similar to traditional programming environments. 

We have previously presented a corpus of 249,376 spreadsheets, called \textsc{Fuse}~\cite{barik2015}, extracted from the over 26.83 billion web pages in the Common Crawl\footnote{The Common Crawl non-profit organization provides this index to companies and individuals at no cost for the purpose of research and analysis. For more information, see \url{http://www.commoncrawl.org}.} index.
This corpus gives researchers the potential to extract unprecedented insight into how end-user use such an ubiquitous form of programming.


\section{Proposed MSR Challenge}
Previous MSR challenges have been predominantly focused on the types of activities performed by professional programmers (see Table \ref{tab:datasources}). 
For example, years 2006 through 2011 of MSR challenges focused on source code repositories, bug data, and mailing lists for particular software projects. 
In 2012, the MSR challenge was extended to a different domain --- mobile devices --- but the sources of data considered were still essential change logs and bug reports. 

In more recent years, to increase diversity the MSR challenges have been expanding their datasets to broader information sources, such as Stack Overflow.
Well-recieved mining papers, like the one presented by Chowdhury and Hindle \cite{chowdhury2015}, have tended to draw on additional, diverse datasets like YouTube comments.

End-user programmers outnumber professional programmers by an order of magnitude~\cite{Scaffidi2005}.
Given their role and importance will only increase with time, our proposed MSR challenge asks participants to obtain insights about end-user programmers and the artifacts that they produce using spreadsheets. 
%Can we identify the types of artifacts that they create? 
%What type of features do they use to develop these artifacts? 
%What type of mistakes do they make?
%These are some of the questions (see Section \ref{how_to_use_fuse}) we hope challenge participants can answer to help improve the code quality and experience for the millions of end-user programmers.
%
%We propose that spreadsheet corpora, such as \textsc{Fuse}, serve as the data source for next year's challenge. 
First, we think that such a challenge would increase the diversity of the MSR community, by garnering interesting from researchers in the end-user communities. 
Second, spreadsheets may serve as a data source to allow software engineering researchers to try their tools and techniques, such as static analysis, on a very different domain than what they typically study. 
Finally, we hope that this MSR challenge will inspire new tools and generate new insights about end-user programmers, and that such findings will drive the research direction of future end-user research.

%Todo(tbarik): Diversity adds a bunch of stuff.  Stack overflow gave Pat his aging developers talk.  Youtube comments + github comments let him do interesting things

% 2006: http://msr.uwaterloo.ca/challenge/
% 2007: http://msr.uwaterloo.ca/msr2007/challenge/
% 2008: http://msr.uwaterloo.ca/msr2008/
% 2009: http://msr.uwaterloo.ca/msr2009/
% 2010: http://msr.uwaterloo.ca/msr2010/index.html
% 2011: http://2011.msrconf.org/msr-challenge.html
% 2012: http://2012.msrconf.org/challenge.php
% 2013: http://2013.msrconf.org/challenge.php
% 2014: http://2014.msrconf.org/challenge.php
% 2015: http://2015.msrconf.org/challenge.php
\begin{table}[!t]
\caption{Previous MSR Challenge Data Sources\label{tab:datasources}}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{ll}
\toprule
\textbf{\textsc{Year}} & \textbf{Data Source}\\
\midrule
2006 & PostgreSQL and ArgoUML\\
2007 & Eclipse and Firefox\\
2008 & Eclipse\\
2009 & GNOME Desktop\\
2010 & FreeBSD, Debian, and GNOME\\
2011 & Eclipse and NetBeans, Firefox and Chrome\\
2012 & Android\\
2013 & Stack Overflow\\
2014 & GitHub\\
2015 & Stack Overflow\\
\bottomrule
\end{tabular}
\end{table}



\section{The Origins of \textsc{Fuse}}

While we point to our original data paper for an extensive description of how the data was obtained, we will briefly rehash the procedure in this section.

\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Comparison of \textsc{Fuse} and other spreadsheet corpora\label{tab:corpora}}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{llllll}
\toprule
 & \textbf{\textsc{Fuse}} & \textbf{EUSES} & \textbf{Enron} \\
\midrule
Size ($n$) & 249,376 & 6,000 & 15,570\\
Space (GB) & 87  & 0.64 & 23.3\\
Access & All & Researchers & All \\
Unique formulas & 894,361 & 84,004 & 784,380 \\
Extendable & Yes & Yes & No\\
Framework & Hadoop & Excel/VBA & Scantool \\
Scalable & Yes & No & No\\
Collection period & 2013-2014 & 2006 & 2006 \\
Primary origin & CC & Google & Enron \\
Binaries & Yes  & Yes & Yes \\
Metadata & Yes & No & No \\
Domains & 4,318 & ??? & - \\
Distinct functions & 219 & 209 & 139 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{The Common Crawl Web Index}

We extracted \textsc{Fuse} from the over 26.83 billion web pages in the Common Crawl web index, which is hosted as one of Amazon EC2's public datasets. 
The Common Crawl non-profit organization provides a monthly index to companies and individuals at no cost for the purpose of research and analysis\footnote{For more information, see \url{http://www.commoncrawl.org}}.
This index contains not only the index of the web pages, but the original content of those web pages as well.
Because we extracted our spreadsheets from the stored original web pages, \textsc{Fuse} is a reproducible dataset.


%todo(tbarik): even after extraction, it is a non-trivial size set 
\subsection{Extracting the spreadsheets from a 2 PB Index}

The process for producing \textsc{Fuse} was completed in 5 steps: \textbf{Match} potential spreadsheets based on MIME-types; \textbf{Extract} the candidate spreadsheets from the compressed web index; \textbf{Filter} non-spreadsheets out based on file contents; \textbf{Analyze} the spreadsheets to create metadata from four sources; and, finally, \textbf{Merge} the metadata together.


\section{Obtaining \textsc{Fuse} and Making Sense of It}

The data can be directly loaded from go.ncsu.edu/fuse.

The result, \textsc{Fuse}, is characterized through two, hierarchical datasets: a Web Analysis dataset, and a Binary Analysis dataset.

\textbf{Web Analysis:} This dataset contains \urlcount{} spreadsheet-related URLs and HTTP responses. 292,043 of these responses point to a unique URL, and the top domain is \texttt{.org} (29.5\%), followed by \texttt{.gov} (27.7\%). 
The analysis contains 6,316 distinct domain names. Unfortunately, relying solely on Web Analysis for spreadsheets will not result in a reproducible corpus, as the Internet is always in flux.

\textbf{Binary Analysis:}  Due to the fact that the content pointed to by the links in Web Analysis can change over time, the Binary Analysis dataset contains \xlscount{} unique spreadsheets, extracted directly from the raw data contained within Common Crawl archives, rather than the Internet. 
Since each monthly Common Crawl archive is a permanent snapshot in time, the spreadsheets in Binary Analysis are always reproducible.

We augmented the Binary Analysis to also contain some metadata to help index the spreadsheets.
For our corpus, we augmented the JSON record with three plugins: 
\texttt{InternetDomain}, which uses the Google Guava\footnote{\url{https://github.com/google/guava}} library to extract domain-related information from the \texttt{WARC-Target-URI};
Apache \texttt{POI}\footnote{\url{http://poi.apache.org/}}, which obtains metrics on the content of the spreadsheets, such as the use of functions;
and \texttt{LingPipe}\footnote{\url{http://alias-i.com/lingpipe/}}, which extracts language-related information from the spreadsheet. 

The \texttt{Tika} element contains four fields related to the file metadata, which include the MIME type, a best-guess file extension, a SHA-1 signature, and the length in bytes of the spreadsheet.

The \texttt{InternetDomain} element is useful for analysis relating to the origin of a spreadsheet. It uses the \texttt{WARC-Target-URI} and extracts the host (e.g., \texttt{www.example.org}), the top private domain (e.g., \texttt{example.org}), and a public suffix\footnote{\url{https://publicsuffix.org/}} (e.g., \texttt{org}).

Next, we provide an Alias-i \texttt{LingPipe} element, which extracts the token stream (keywords) from spreadsheets, lowercases these tokens, removes English stop words (such as `a' or `the'), and filters out non-words (such as numbers).

Finally, to get a high-level overview of the content of the spreadsheets, we used Apache \texttt{POI} to gather spreadsheet metrics. There are over 450 such metrics, which include the number of times a given Excel function (such as \texttt{SUM} or \texttt{VLOOKUP}) is used, the total number of input cells (i.e., cells that are not formulas), the number of numeric input cells, and the most common formula used. 

%If you really need to, you can take the deduped binary spreadsheet and expand them back into their full set. The first column of the fuse-dedup.map-dec2014.txt  file contains the WARC-Record-ID of the original spreadsheet. The second column contains the equivalent deduped WARC-Record-ID. Thus, for each record, one must simply copy the deduped file to the expanded file. There are any number of ways to do this:

%cat cc.dedup.map.txt | \ 
%awk '{print "src/" $2 "\n" "dst/" $1 "\n"}' | \ 
%xargs -n 2 cp

%\todo{tbarik} : look at the last 10 years of data challenges and make a table to show it's a bunch of the same

\section{Motivation for why the dataset is appropriate}

We believe this dataset is a good fit for MSR 2016 data challenge because in our initial analysis of the data, we noticed clues that these spreadsheets contain non-trivial programming, most of which is done by end-users.
[citation for end users being a non-neglible portion of the Software development community].  
Fuse offers a chance to shed some deep insights into the type of code end users find themselves needing to write.
For example, IF was the most common function used across all \xlscount{} spreadsheets, more common than SUM, AVERAGE, even basic operations like addition, subtraction, etc.
In 29.6\% of these uses of IF, formulas contained at least two IF clauses, in some cases nested 20 or more times in a single cell.
End user programmers are making actual software, and now we have a large corpus to see how they do it.

One additional motivation for why we think this dataset would make a good challenge paper is that the dataset is large enough that we, the curators, don't even have a good sense of what it entails.
This challenge will require participants to apply clever machine learning techniques at scale, which is something previous corpora haven't necesitated (The original Euses was analyzed on a laptop computer over the course of a few weeks with an Excel VB script.)
Additionally, we are fortunate to have these previous corpora to compare Fuse to (Euses, Enron), which opens up a dynamic range of analysis.


\section{A sample list of ways the data could be used}
\label{how_to_use_fuse}
There are many ways spreadsheets have been studied in the past.  
In this section, we present a few of them as well as a few of our own with the hope of sparking the minds of the creative MSR community.

One similarity that spreadsheet corpora have is that not every spreadsheet uses formulas.  
In fact, the proportion of formula-containing spreadsheets is typically less than half.
This has been noticed by even the developers of Excel:
\begin{displayquote}
Everybody thought of Excel as a financial modeling application, [but] we visited dozens of Excel customers, and did not see anyone using Excel to actually perform what you would call `calculations.' Almost all of them were using Excel because it was a convenient way to create a table.

 --- Joel Spolsky~\cite{JoelOnSoftware}
\end{displayquote}
The open question we have is \textbf{Can you detect spreadsheets that could use formulas but don't using techniques like machine learning?}
As far as we know, no one has directly answered this.

Several researchers have investigated techniques of determining the quality of spreadsheets through static analysis, using theoretically similar approaches to their software engineering counterparts.
For example, Cunha and colleagues have created a quality model of Spreadsheets~\cite{Cunha2012}.
The work by Pinzger and others introduced the idea of code smells to spreadsheets\cite{Pinzger2012}.
Most recently, Jannach and colleagues came up with a technique for automatically finding and fixing spreadsheet errors~\cite{jannach2014}.
One downside to these recent approaches is that they all relied on the well-known EUSES corpus~\cite{Fisher2005}, which contained about 5,000 spreadsheets from 2005.
It would be interesting to \textbf{replicate findings of these static analysis tools on \textsc{Fuse}}, and \textbf{to expand these techniques to include modern spreadsheet tools and functions}.

We found some spreadsheets that changed at least once over the course of the Common Crawl.
The fact that \textsc{Fuse} has this temporal component opens the door to literally a new dimension of analysis, such as \textbf{How do spreadsheets and their formulas change over time?}
We anticipate that researchers will be able to use a spreadsheet-diffing tool like SheetDiff created by Chambers and colleagues~\cite{chambers2010} to assist with answering this family of research questions.

The final set of research questions digs into the structure of spreadsheets and clustering or otherwise drawing conclusions about the set.
The work by Abraham and colleagues \cite{Abraham2006} looked at \textbf{what can you extrapolate about a spreadsheet based on its structure}, such as "This looks like a gradebook and this looks like an inventory".
Along this vein, \textbf{How likely are formulas to co-exist across sheets?} 
\textbf{Do some functions cluster together? }
\textbf{Can we do collaborative filtering or a similar technique and make recommendations to users? }
Small examples like those created by Le and Gulwani's Flash Extraction technique~\cite{le2014}.  

Discovering new metrics that can be easily added to spreadsheet corpora to help tell us more about spreadsheets (augment apache POI).

Signal to noise ratio: How can you determine ``interesting'' spreadsheets for some definition of ``interesting''

\section{Any research that has used the dataset or a similar dataset up to this point}
We made the dataset available in March 2015.  

Since then, we have had tens of downloads and are currently aware of at least three researchers or research groups using Fuse.  
\footnote{Links to known researchers or research labs using Fuse: \url{http://emeryberger.com/research/}, \url{http://research.csc.ncsu.edu/dlf/}, and \url{http://www.felienne.com/}}




\section{Any other details the authors feel is appropriate}
Don't put this in ACM.  Make all references to the MSR data paper.

We intend this paper to be instructions for the MSR data challenge.  
However, we request that our msr 2015 paper be cited as a part of the challenge
\section{Short bio of the authors}

Kevin Lubick is struggling to be a PhD student at North Carolina State University.  He has a part time job as a soccer referee, where the 50\% approval rate is a drastic improvement over his abysmal paper acceptance rates.  He clings to Titus Barik for publications and a resemblence of competency.

Titus Barik is a PhD student at North Carolina State University, a researcher at ABB corporate research and a diligent father - that is to say, tired and cranky.  Due to a glitch in the Human Resources department, he was accidentally hired to be a 2015 intern at Microsoft Research.  He started researching spreadsheet corpora after accidentally replying to an email that mentioned free donuts.  He believes he is advised by Emerson Murphy-Hill.

Emerson Murphy-Hill is a professor at North Carolina State University.  After several years of hard work, he received tenure, but was dismayed to find out no one cares.  He is disillusioned to believe he advises both Titus and Kevin His favorite food is bananas, an interest he shares with his daughter Zuri.

\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{library}

\end{document}
