\documentclass[conference]{IEEEtran}
\usepackage{IEEEpreamble}

\usepackage{url}
\usepackage{booktabs}
\usepackage{csquotes}

\newcommand{\urlcount}{2,127,284}
\newcommand{\xlscount}{249,376}
\newcommand{\urlcountunique}{292,043}

\begin{document}


\title{Using Fuse for the MSR 2016 Data Challenge}

\author{
\IEEEauthorblockN{Kevin Lubick\IEEEauthorrefmark{1},
Titus Barik\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, 
Emerson Murphy-Hill\IEEEauthorrefmark{1}}
\IEEEauthorblockA{
\IEEEauthorrefmark{1}North Carolina State University, Raleigh, North Carolina, USA\\
\IEEEauthorrefmark{2}ABB Corporate Research, Raleigh, North Carolina, USA\\
 kjlubick@ncsu.edu, titus.barik@us.abb.com, emerson@csc.ncsu.edu
}}


\maketitle

\begin{abstract}
We are proposing the \textsc{Fuse} corpus to be used as the dataset for the MSR 2016 data challenge.
The \textsc{Fuse} corpus was presented at the MSR 2015 data showcase, containing \xlscount{} unique spreadsheets --- an order of magnitude larger than previous corpora.
In this paper we make a case for how this diverse corpus of spreadsheets will offer large benefits to the software engineering and research communities.
We conclude with some promising lines of research \textsc{Fuse} makes possible and give an overview of the tools we provide to aid researchers participating in the challenge.

\end{abstract}


\IEEEpeerreviewmaketitle

\section{Introduction}

End-user programmers today constitute a broad class of users, including teachers, accountants, administrators, managers, research scientists, and even children~\cite{Ko2011}.
Although these users are typically not professional software developers, their roles routinely involve computational tasks that, in many ways, are similar to those of developers --- not just in activity, but also in their underlying cognitive demands on users~\cite{Blackwell2002}. 

\emph{Spreadsheets} are perhaps the most ubiquitous form of end-user programming software~\cite{Scaffidi2005}. \emph{Cells} within the tables of these spreadsheets are augmented with computation, such as expressions, functions, and macros~\cite{Nardi1990}. 
This interplay between presentation and computation within the spreadsheet environment has garnered significant interest from the software engineering research community~\cite{Burnett2009}. 
Researchers have adopted techniques and approaches to studying errors~\cite{Powell2008}, code smells~\cite{Pinzger2012}, and refactoring in spreadsheets~\cite{Badame2012}, similar to traditional programming environments. 

We have previously presented a corpus of 249,376 spreadsheets, called \textsc{Fuse}~\cite{barik2015}, extracted from the over 26.83 billion web pages in the Common Crawl index.
This corpus gives researchers the potential to gain insight into how end-user use such an ubiquitous form of programming at a scale not possible before.


\section{Proposed MSR Challenge}
Previous MSR challenges have been predominantly focused on the types of activities performed by professional programmers (see Table \ref{tab:datasources}). 
For example, years 2006 through 2011 of MSR challenges focused on source code repositories, bug data, and mailing lists for particular software projects. 
In 2012, the MSR challenge was extended to a different domain --- mobile devices --- but the sources of data considered were still essentially change logs and bug reports. 

In more recent years, the MSR challenges have been expanding their datasets to more diverse information sources, such as Stack Overflow.
Well-received mining papers, like the one presented by Chowdhury and Hindle \cite{chowdhury2015}, tend go further and incorporate additional diverse datasets like YouTube comments.

End-user programmers outnumber professional programmers by an order of magnitude~\cite{Scaffidi2005}.
Given their role and importance will only increase with time, our proposed MSR challenge asks participants to obtain insights about end-user programmers and the artifacts that they produce using spreadsheets. 
%Can we identify the types of artifacts that they create? 
%What type of features do they use to develop these artifacts? 
%What type of mistakes do they make?
%These are some of the questions (see Section \ref{how_to_use_fuse}) we hope challenge participants can answer to help improve the code quality and experience for the millions of end-user programmers.
%
%We propose that spreadsheet corpora, such as \textsc{Fuse}, serve as the data source for next year's challenge. 
First, we think that such a challenge would increase the diversity of the MSR community, by garnering interest from researchers in the end-user communities. 
Second, spreadsheets may serve as a data source to allow software engineering researchers to try their tools and techniques, such as static analysis, on a very different domain than what they typically study. 
Finally, we hope that this MSR challenge will inspire new tools and generate new insights about end-user programmers, and that such findings will drive the directions of a broad range of software engineering research.

%Todo(tbarik): Diversity adds a bunch of stuff.  Stack overflow gave Pat his aging developers talk.  Youtube comments + github comments let him do interesting things

% 2006: http://msr.uwaterloo.ca/challenge/
% 2007: http://msr.uwaterloo.ca/msr2007/challenge/
% 2008: http://msr.uwaterloo.ca/msr2008/
% 2009: http://msr.uwaterloo.ca/msr2009/
% 2010: http://msr.uwaterloo.ca/msr2010/index.html
% 2011: http://2011.msrconf.org/msr-challenge.html
% 2012: http://2012.msrconf.org/challenge.php
% 2013: http://2013.msrconf.org/challenge.php
% 2014: http://2014.msrconf.org/challenge.php
% 2015: http://2015.msrconf.org/challenge.php
\begin{table}[!t]
\caption{Previous MSR Challenge Data Sources\label{tab:datasources}}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{ll}
\toprule
\textbf{\textsc{Year}} & \textbf{Data Source}\\
\midrule
2006 & PostgreSQL and ArgoUML\\
2007 & Eclipse and Firefox\\
2008 & Eclipse\\
2009 & GNOME Desktop\\
2010 & FreeBSD, Debian, and GNOME\\
2011 & Eclipse and NetBeans, Firefox and Chrome\\
2012 & Android\\
2013 & Stack Overflow\\
2014 & GitHub\\
2015 & Stack Overflow\\
\bottomrule
\end{tabular}
\end{table}



\section{The Contents of \textsc{Fuse}}

In this section, we summarize the contents of the \textsc{Fuse} corpus.
For readers curious on the details of the extraction process that took about 60,000 instance hours, see our MSR data paper~\cite{barik2015}.

The primary contribution of \textsc{Fuse} is the \xlscount{} unique spreadsheets extracted from the Common Crawl web index\footnote{The Common Crawl non-profit organization provides this index to companies and individuals at no cost for the purpose of research and analysis. For more information, see \url{http://www.commoncrawl.org}.} .
The second component is a myriad of metadata about these spreadsheets, which is detailed below.
Finally, we released the \urlcount{} web URLs that returned spreadsheet-related MIME types, of which the URLs that returned actual spreadsheets are a subset.

%todo(tbarik): even after extraction, it is a non-trivial size set 

For our corpus, we augmented the JSON web record returned from Common Crawl with additional metadata from four \textit{plugins}: 
\texttt{Apache Tika}\footnote{\url{https://tika.apache.org/}}, which contbutes file metadata including the MIME type, a best-guess file extension, a SHA-1 signature, and the length in bytes of the spreadsheet;
\texttt{InternetDomain}, which uses the Google Guava\footnote{\url{https://github.com/google/guava}} library to extract domain-related information from the \texttt{WARC-Target-URI};
\texttt{LingPipe}\footnote{\url{http://alias-i.com/lingpipe/}}, which extracts language-related information from the spreadsheet. 
and \texttt{Apache POI}\footnote{\url{http://poi.apache.org/}}, which obtains metrics on the content of the spreadsheets, such as the use of functions;

The last two elements are provided as a way for researchers to query for spreadsheets of the most interest, and we detail them a bit more here.
The \texttt{LingPipe} plugin extracts the token stream (keywords) from spreadsheets, lowercases these tokens, removes English stop words (such as `a' or `the'), and filters out non-words (such as numbers).
As an example, these tokens could be used to locate financial spreadsheets by looking for words such as ``accounts receivable'', ``depreciation'' and so on.

The \texttt{Apache POI} plugin gathers preadsheet metrics to get a high-level overview of the content of the spreadsheets.
There are over 450 such metrics, which include the number of times a given Excel function (such as \texttt{SUM} or \texttt{VLOOKUP}) is used, the total number of input cells (i.e., cells that are not formulas), the number of numeric input cells, and the most common formula used. 

% \section{Motivation for why the dataset is appropriate}
% 
% We believe this dataset is a good fit for MSR 2016 data challenge because in our initial analysis of the data, we noticed clues that these spreadsheets contain non-trivial programming, most of which is done by end-users.
% [citation for end users being a non-neglible portion of the Software development community].  
% Fuse offers a chance to shed some deep insights into the type of code end users find themselves needing to write.
% For example, IF was the most common function used across all \xlscount{} spreadsheets, more common than SUM, AVERAGE, even basic operations like addition, subtraction, etc.
% In 29.6\% of these uses of IF, formulas contained at least two IF clauses, in some cases nested 20 or more times in a single cell.
% End user programmers are making actual software, and now we have a large corpus to see how they do it.
% 
% One additional motivation for why we think this dataset would make a good challenge paper is that the dataset is large enough that we, the curators, don't even have a good sense of what it entails.
% This challenge will require participants to apply clever machine learning techniques at scale, which is something previous corpora haven't necesitated (The original Euses was analyzed on a laptop computer over the course of a few weeks with an Excel VB script.)
% Additionally, we are fortunate to have these previous corpora to compare Fuse to (Euses, Enron), which opens up a dynamic range of analysis.


\section{Potential Spreadsheet Research Questions}
\label{how_to_use_fuse}
There are many ways spreadsheets have been studied in the past.  
In this section, we present a few of them as well as a few of our own with the hope of sparking the minds of the creative MSR community.

One similarity found among several spreadsheet corpora (Table \ref{tab:corpora}) is that not every spreadsheet uses formulas.  
In fact, the proportion of formula-containing spreadsheets is typically less than half.
This has been noticed by even the developers of Excel:
\begin{displayquote}
Everybody thought of Excel as a financial modeling application, [but] we visited dozens of Excel customers, and did not see anyone using Excel to actually perform what you would call `calculations.' Almost all of them were using Excel because it was a convenient way to create a table.

 --- Joel Spolsky~\cite{JoelOnSoftware}
\end{displayquote}
The open question we have is \textbf{Can you detect spreadsheets that could use formulas but don't using techniques like machine learning?}
As far as we know, no one has directly answered this.

Several end-user researchers have investigated techniques of determining the quality of spreadsheets through static analysis, using similar approaches to their software engineering counterparts.
For example, Cunha and colleagues have created a quality model of spreadsheets~\cite{Cunha2012}
and the work by Pinzger and others introduced the idea of code smells to spreadsheets~\cite{Pinzger2012}.
Most recently, Jannach and colleagues came up with a technique for automatically finding and fixing spreadsheet errors~\cite{jannach2014}.
One downside to these recent approaches is that they all relied on the well-known EUSES corpus~\cite{Fisher2005}, which contains about 5,000 spreadsheets circa 2005.
It would be interesting to \textbf{replicate findings of these static analysis tools on \textsc{Fuse}}, and \textbf{to expand these techniques to include modern spreadsheet tools and functions}.

We found some spreadsheets that changed at least once over the course of the Common Crawl.
The fact that \textsc{Fuse} has this temporal component opens the door to literally a new dimension of analysis, such as \textbf{How do spreadsheets and their formulas change over time?}
We anticipate that researchers will be able to use a spreadsheet-diffing tool like SheetDiff created by Chambers and colleagues~\cite{chambers2010} to assist with answering this family of research questions.

\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Comparison of \textsc{Fuse} and other spreadsheet corpora\label{tab:corpora}}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{llllll}
\toprule
 & \textbf{\textsc{Fuse}} & \textbf{EUSES} & \textbf{Enron} \\
\midrule
Size ($n$) & 249,376 & 6,000 & 15,570\\   %todo(kjlubick): Exact Euses number from paper
Space (GB) & 87  & 0.64 & 23.3\\
Access & All & Researchers & All \\
Unique formulas & 894,361 & 84,004 & 784,380 \\
Extendable & Yes & Yes & No\\
Framework & Hadoop & Excel/VBA & Scantool \\
Scalable & Yes & No & No\\
Collection period & 2013-2014 & 2006 & 2006 \\
Primary origin & CC & Google & Enron \\
Binaries & Yes  & Yes & Yes \\
Metadata & Yes & No & No \\
Domains & 4,318 & ??? & - \\
Distinct functions & 219 & 209 & 139 \\
\bottomrule
\end{tabular}
\end{table}

The final set of research questions digs into the structure of spreadsheets and clustering or otherwise drawing conclusions about the set.
The work by Abraham and colleagues \cite{Abraham2006} looked at \textbf{what can you extrapolate about a spreadsheet based on its structure}, such as "This looks like a gradebook and this looks like an inventory".
Along this vein, \textbf{How likely are formulas to co-exist across sheets?} 
\textbf{Do some functions cluster together and can we make recommendations based on this? }
\textbf{Are there metrics that can be used to identify ``interesting'' spreadsheets, for some definition of interesting?}
The space of questions is enormous and we end the list here, having inspired potential challenge participants.


\section{The use of other spreadsheet corpora}
Keeping with the spirit of diverse submissions, we would also like to point to two other spreadsheet corpora that participants may consider including in their analysis.
We summarize these corpora in Table \ref{tab:corpora}.
The EUSES corpus~\cite{Fisher2005} was created by scraping Google search queries for spreadsheets and keywords like ``finance''.
As a part of the Enron trial, over 15,000 spreadsheets were included in the released emails, which were extracted to make the Enron corpus~\cite{Hermans2015}.

These three corpora offer three different viewpoints into the user of spreadsheets --- for example, the Enron corpus offers insight into professional, financial spreadsheets.
Our challenge would not require the use of EUSES or Enron corpora, but we would like participants to be aware of them and be free to use them to compare or contrast to \textsc{Fuse}.



\section{Useful Analysis notes for Participants}

In addition to releasing \textsc{Fuse}, we also released the source code of the tools we used to create the corpus.
Of note, we used Apache POI as the main library for analyzing the spreadsheets, as it is fast, written in Java, open source and works well on most spreadsheet formats.
One thing to keep in mind is that Apache POI uses up a fair amount of memory - to analyze spreadsheets that are 3MB or bigger requires at least 2 GB of heap space to avoid out-of-memory  errors.

One potential concern participants may have with our dataset is it requiring an unreasonable amount of processing power for such a challenge.
We would like to allay those concerns by sharing the fact that creating the metadata for all of the spreadsheets took around 400 instance hours on Amazon EMR, a far more reasonable number than the 60,000 hours creating the corpus required.

Additionally, analysis time can be reduced further by using the metadata we provide to narrow the search space.
For example, participants can load our metadata into MongoDB, query for spreadsheets that contain at least one IF statement, and then analyze only those several thousand spreadsheets.

\section{Past and Present Research involving Fuse}
We made the roughly 8 GB dataset available in March 2015, upon submission of the MSR data paper.  
Since then, we have had tens of downloads and are currently aware of at least three researchers or research groups using Fuse\footnote{Links to known researchers or research labs using Fuse: \url{http://emeryberger.com/research/}, \url{http://research.csc.ncsu.edu/dlf/}, and \url{http://www.felienne.com/}}.




\section{Final notes for challenge committee}
We intend this paper to be the basis for the MSR 2016 data challenge.  
However, we request that our MSR 2015 paper~\cite{barik2015} be cited as a part of the challenge.


\section{Short bio of the authors}

Kevin Lubick is struggling to be a PhD student at North Carolina State University.  He has a part time job as a soccer referee, where the 50\% approval rate is a drastic improvement over his abysmal paper acceptance rates.  He clings to Titus Barik for publications and a resemblence of competency.

Titus Barik is a PhD student at North Carolina State University, a researcher at ABB Corporate Research and a diligent father - that is to say, tired and cranky.  Due to a glitch in the Human Resources department, he was accidentally hired to be a 2015 intern at Microsoft Research.  He started researching spreadsheet corpora after accidentally replying to an email that mentioned free donuts.  He believes he is advised by Emerson Murphy-Hill.

Emerson Murphy-Hill is a professor at North Carolina State University.  After several years of hard work, he received tenure, but was dismayed to find out no one cares.  He is disillusioned to believe he advises both Titus and Kevin. His favorite food is bananas, an interest he shares with his daughter Zuri.

\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{library}

\end{document}
