\documentclass[conference]{IEEEtran}
\usepackage{IEEEpreamble}

\usepackage{booktabs}
\usepackage{blindtext}
\usepackage{sepnum}
\usepackage{url}
\usepackage{listings}
\usepackage{zi4}
% \usepackage{minted}

\usepackage{tikz}

\lstset{
  basicstyle=\small\ttfamily
}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{\textsc{Fuse}: A Reproducable, Extendable, Internet-scale\\Dataset of Spreadsheets}

\author{
\IEEEauthorblockN{Titus Barik\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Kevin Lubick\IEEEauthorrefmark{2}, Justin Smith\IEEEauthorrefmark{2}, John Slankas\IEEEauthorrefmark{2}, Emerson Murphy-Hill\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}ABB Corporate Research, Raleigh, North Carolina, USA\\
\IEEEauthorrefmark{2}North Carolina State University, Raleigh, North Carolina USA\\
titus.barik@us.abb.com, \{kjlubick, jssmit11, jbslanka\}@ncsu.edu, emerson@csc.ncsu.edu
}}

\maketitle

\begin{abstract}
Spreadsheets are perhaps the most ubiquitous, widely used form of end-user programming software. This paper describes a dataset, called \textsc{Fuse}, consisting of metadata information for 719,223 spreadsheet accesses and their corresponding 249,376 binary files from a public web crawl of over 4 billion pages. The resulting dataset offers several useful properties over prior spreadsheet corpora, including reproducibility, extendability, and queryability. The dataset is unencumbered by any license agreements, available to all, and intended for wide usage by end-user software engineering researchers. In this paper, we detail the spreadsheet extraction process, describe the data schema, illustrate example queries to highlight the features of the dataset, and discuss the limitations and challenges of \textsc{Fuse}.
\end{abstract}


\IEEEpeerreviewmaketitle

% Oops: we forgot to handle application/vnd.oasis.opendocument.spreadsheet.

% \textbf{Data papers}. We want to encourage researchers to share their data. Data papers should describe data sets curated by their authors and made available to others. They are expected to be at most 4 pages long and should address the following: description of the data, including its source; methodology used to gather it; description of the schema used to store it, and any limitations and/or challenges of this data set. The data should be made available at the time of submission of the paper for review, but will be considered confidential until publication of the paper. Further details about data papers are available on the conference website. 

% \begin{enumerate}
% \item Description of the data.
% \item Methodology used to gather it
% \item Description of the schema used to store it
% \item Limitations and Challenges of data set
% \end{enumerate}

\section{Introduction}

End-user programmers today constitute a broad class of users, including teachers, accountants, administrators, managers, research scientists, and even children~\cite{Ko2011}.
%
Although these users are typically not professional software developers, their roles routinely involve computational tasks that, in many ways, are similar to those of developers --- not just in activity, but also in their underlying cognitive demands on users~\cite{Blackwell2002}. 

% As Nardi and Miller observe, these non-programmers are not simply under-skilled programmers; they are not programmers at all~\cite{Nardi1990}.

Perhaps the most ubiquitous and widely used form~\cite{Scaffidi2005} of end-user programming software are \emph{spreadsheets}, a table-oriented visual interface that serves as the underlying model for the users' applications~\cite{Nardi1990}. \emph{Cells} within these tables are augmented with computational techniques, such as functions and macros, that are expressive and yet simultaneously shield users from the low-level details of traditional programming~\cite{Nardi1990}.

% \footnote{``Spreadsheets are not just tools for doing “what-if” analysis. They provide a specific data structure: a table. Most Excel users never enter a formula. They use Excel when they need a table. The gridlines are the most important feature of Excel.'' \url{http://www.joelonsoftware.com/items/2012/01/06.html}}.

This unique interplay between presentation and computation within the spreadsheet environment has, unsurprisingly, garnered significant interest from the software engineering research community~\cite{Burnett2009}. In noticing the similarities and differences with traditional programming environments, researchers have adopted techniques and approaches to studying errors~\cite{Pinzger2012}, code smells~\cite{Badame2012}, refactoring~\cite{Abraham2007}, and debugging in spreadsheets~\cite{Powell2008}. For example, Abraham and Erwig exploit the spatial arrangements of tables within spreadsheets, a visual property inapplicable to traditional programming languages, to infer templates that help end-users safely edit spreadsheets~\cite{Abraham2006}.

To better understand end-user activities and design tools to assist end-users, researchers have responded by curating spreadsheet corpora to support spreadsheet studies: among them, EUSES~\cite{Fisher2005}, obtained by simple Google keyword searches and from Oregon State University students and researchers; Enron~\cite{Felienne2015}, extracted from e-mails obtained during legal evidence; and \textsc{Senbazuru}/ClueWeb09~\cite{Chen2013}, obtained from the ClueWeb Web crawl by Cargenie Mellon University.

% http://www.felienne.com/archives/3634
\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Comparison of \textsc{Fuse} and other spreadsheet corpora\label{tab:corpora}}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{llllll}
\toprule
 & \textbf{\textsc{Fuse}} & \textbf{EUSES} & \textbf{Enron} & \textbf{ClueWeb}\\
\midrule
Size ($n$) & 249,376 & 6,000 & 15,570 & 410,554\\
Space (GB) & b & 0.64 & 23.3 & 110\\
Access & All & Researchers & All & All\\
Unique formulas & 894361 & 693266 & 84004 & --\\
Extendable & Yes & Not scalable & No & Yes\\
Framework & Hadoop & Excel/VBA & Scantool & \textsc{--}\\
Time Period & 2006 & 2006 & 2006 & 2009\\
Origin & CC & Google & Enron & ClueWeb09\\
Distinct functions & 219 & 209 & 139 & --\\
\bottomrule
\end{tabular}
\end{table}

This paper presents another spreadsheet corpus, called \textsc{Fuse}, extracted from the over 4.2 billion web pages in the Common Crawl index. We believe that \textsc{Fuse} offers several useful traits not found in previous corpora. First, unlike EUSES or ClueWeb, \textsc{Fuse} is fully reproducible, as it is derived from archived snapshots containing both HTTP response data and the associated binary data. In contrast, EUSES provides binary spreadsheets but no origin information indicates the URL from which these spreadsheets were obtained. Similarly, ClueWeb provides only the URL of the spreadsheet. However, since the Internet is not a static entity, many of these URLs do not point to the same content as it did when the crawl was originally performed, and others are no longer available. Second, unlike EUSES or Enron, our corpus supports systematic updating as new crawls are added to the Common Crawl. Third, and perhaps most importantly, our corpus is the only one to support querying of spreadsheet metadata, facilitated by a JSON document associated with each spreadsheet. A comparison of these and other differences between corpora are summarized in Table~\ref{tab:corpora}.

% TODO(tbarik): Just for reference. Remove for submission.
% The text height is \the\columnwidth -- should 252.0pt or 3.5in.

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{pipeline}
\caption{The MapReduce pipeline for extracting spreadsheets and associated spreadsheet metadata from Common Crawl.\label{fig:mrpipeline}}
\end{figure*}


The contributions of this paper are:

 \begin{itemize}
 \item A corpus of metadata and binary spreadsheets extracted from public web sites through the Common Crawl archive, made accessible to the research community.\footnote{The corpus metdata, binary spreadsheets, tools, and other documentation can be obtained at \url{http://go.barik.net/fuse}. This URL is currently intended only for examination by the review committee.}
 \item A modular, open-source pipeline of tools, implemented using MapReduce. Our tool supports scalability from the ground up, and can cost-effectively process over 1 million spreadsheets in under an hour.
 \item A mixin system that enables researches to augment our analysis with their own data, using a schema-free, document-centric JSON format, supported by many popular database technologies.
 \end{itemize}

% \begin{figure}[!t]
% \centering
% \includegraphics[width=\columnwidth]{corpora}
% \caption{Comparison matrix of available spreadsheet corpora. How does \textsc{Fuse} stack up?}
% \label{fig:corpora}
% \end{figure}


% Improve display of JSON files
% http://tex.aspcode.net/view/635399273629833626148902/how-to-improve-listings-display-of-json-files

% TODO(tbarik)
%  This WAT file is okay, but the corresponding WARC record is corrupt:
% Put an example of this issue in the methodology.
%  {"WARC-Date":"2014-09-22T12:11:35Z","WARC-Record-ID":"<urn:uuid:0d670ac1-319f-42b0-89bf-7c52dd51e1dd>","Content-Length":1385,"WARC-Target-URI":"http://www.medcom.dk/dwn1514.xls","WARC-Content-Type":"application/json","Content":{"Envelope":{"Format":"WARC","WARC-Header-Length":"350","Block-Digest":"sha1:6JCTYB2KGCZ5YX63EFLECRMXB4KUBOQB","Actual-Content-Length":"263","WARC-Header-Metadata":{"WARC-Type":"request","WARC-Date":"2014-09-22T12:11:35Z","WARC-Warcinfo-ID":"<urn:uuid:1c4997f8-bc87-4ade-8efd-47f4cb8ce2a6>","Content-Length":"263","WARC-Record-ID":"<urn:uuid:81815bbd-d47d-4755-8e41-d820a1d05043>","WARC-Target-URI":"http://www.medcom.dk/dwn1514.xls","WARC-IP-Address":"91.193.139.41","Content-Type":"application/http; msgtype=request"},"Payload-Metadata":{"Trailing-Slop-Length":"4","HTTP-Request-Metadata":{"Headers":{"Accept-Language":"en-us,en-gb,en;q=0.7,*;q=0.3","Host":"www.medcom.dk","Accept-Encoding":"x-gzip, gzip, deflate","User-Agent":"CCBot/2.0 (http://commoncrawl.org/faq/)","Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"},"Headers-Length":"261","Entity-Length":"0","Entity-Trailing-Slop-Bytes":"0","Request-Message":{"Method":"GET","Version":"HTTP/1.0","Path":"/dwn1514.xls"},"Entity-Digest":"sha1:3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ"},"Actual-Content-Type":"application/http; msgtype=request"}},"Container":{"Compressed":true,"Gzip-Metadata":{"Footer-Length":"8","Deflate-Length":"418","Header-Length":"10","Inflated-CRC":"505390695","Inflated-Length":"617"},"Offset":"654053188","Filename":"CC-MAIN-20140914011217-00240-ip-10-234-18-248.ec2.internal.warc.gz"}},"WARC-Refers-To":"<urn:uuid:81815bbd-d47d-4755-8e41-d820a1d05043>","Path":"common-crawl/crawl-data/CC-MAIN-2014-41/segments/1410657137046.16/wat/CC-MAIN-20140914011217-00240-ip-10-234-18-248.ec2.internal.warc.wat.gz"}

\section{Methodology}

% Web pages: 26.83 billion.
% Petabytes: 1.9 PB
% Packed: 423.8 TB.

The Common Crawl\footnote{\url{http://commoncrawl.org}} organization is 501(c)(3) non-profit dedicated to providing a copy of the Internet, and democratizing the data so that it is accessible to everyone. 
%
Of specific interest to us is that the corpus contains not only the metadata of web pages, but also the raw content of resource, including binaries. Importantly, these crawls occur periodically, at a frequency of approximately once per month. 

The Common Crawl is available as a public data set on Amazon, and crawl data is stored on Simple Storage Service (S3) as a set of WARC files, store the raw crawl data, and a corresponding WAT file, which stores the computed metadata for the for the WARC file. Essentially, each WAT file contains JSON-formatted records that act as an index into the WARC raw data. That is, each record contains a globally unique identifier, which we call a \texttt{WARC-Record-ID}, and a reference to a WARC filename, offset, and length. Because S3 supports object ranges, it then becomes possible to download the raw content of a single record without downloading an entire WARC file.

We considered spreadsheets from the period of Summer 2013--December 2014, which consists of 26.83 billion web pages, compressed as 423.8 TB (1.9 PB uncompressed). To support parallelization, this data is split into 481,427 segments, such that segment requests can be computed independently by a task node in a cluster. We extracted the spreadsheets using the Amazon Elastic MapReduce service, to take advantage of network co-location.

 
% TODO(tbarik): Copy Mixin and JsonMerge, not shown, but utility tool to easily copy sets and merge them.

\subsection{Hadoop MapReduce Pipeline}


Our overall framework is illustrated in Figure~\ref{fig:mrpipeline}, and consists of five MapReduce tasks that comprise a pipeline. In this section, we consider each of the stages in this pipeline.

\subsubsection{Crawl} 

The first stage of the pipeline is also the expensive computationally, because it requires that we traverse every JSON metadata record in the 481,427 WAT segments and heuristically tag spreadsheet-related records, which we call candidate spreadsheets. This is a heuristic process because cannot know for sure that a record is actually a spreadsheet until we inspect the corresponding WARC file. First, we check if the HTTP response payload \texttt{Content-Type} field corresponds to one of seven spreadsheet MINE types, as listed in MSDN. However, some records contain a generic binary \texttt{Content-Type} of \texttt{application/octet-stream}, in which case \texttt{Content-Disposition} is checked via a file pattern matching ``\texttt{.xls*}''. If either of these conditions are true, we save the record using the \texttt{WARC-Record-ID} as this key. This key is propagated through the pipeline.


The crawl filters through the some 26.83 billion records and identifies 2,127,284 candidate spreadsheets. This stage requires approximately 55,000 normalized instance hours\footnote{A normalized instance hour is the amount of computation it would require for \texttt{m1.small} compute node to complete the task.} to process.

\subsubsection{Extract} 

In this stage of the pipeline, the extract process loads the 2,127,284 candidate spreadsheets records. Using the Filename, Offset, and Deflate-Length fields of the record, the corresponding WAT record is extracted into memory. The WARC record is then stripped of its header information (e.g., the HTTP response), and the remaining content is saved to S3, again using the \texttt{WARC-Record-ID} from the WAT file as the key. Theoretically, this process should yield the same number of records as crawl stage; however, five records had corrupted \texttt{gzip} entries, yielding 2,127,279 candidate spreadsheets.

This stage requires approximately 1000 normalized instance hours to complete.

\subsubsection{Filter} 

The third stage of the pipeline, filter, checks the extracted file and tag those that are actually spreadsheets. Using Apache Tika, this stage uses Tika's built-in MimeType detector, which returns the actual Content-Type of the file. If this result is one of the spreadsheet content types, the record is retained. During this stage, we also compute the length (in bytes) of the spreadsheet, identify the most appropriate file extension (e.g., ``.xlsx''), and generate a SHA-1 digest of the spreadsheet content. 

At this stage, 719,223 spreadsheets are retained in the pipeline, although many of these may be duplicates. This stage requires 420 normalized instance hours to complete.

\subsubsection{Mixin} 

The fourth stage of the pipeline is actually a meta-stage, in which researchers can augment the framework with their own analysis, which we call \emph{mixins}. For our dataset, we augment the JSON document with three mixins: \texttt{InternetDomain}, which uses the Google guava library to extracts domain-related information from the \texttt{WARC-Target-URI}. A second Apache POI analysis extracts relevant information on the content of the spreadsheets, such as the use of functions. A third analysis using LingPipe extracts language-related information from the spreadsheet. These JSON records are all saved to S3 by their \texttt{WARC-Record-ID}. For various reasons, not all APIs can analyze all spreadsheets, even when they open in Microsoft Excel. This stage requires about 200 normalized instance hours per mixin.

\subsubsection{Merge} 

The final stage of the pipeline simply takes the resulting JSON files from all previous stages and combines them with the original WAT record to facilitate downstream analysis. The stage requires approximately 130 normalized instance hours for each mixin.

% https://publicsuffix.org/list/

% sebastien paper for other metrics a look inside common crawl
% http://blog.commoncrawl.org/2013/08/a-look-inside-common-crawls-210tb-2012-web-corpus/
% domain parsing is done using 

% https://publicsuffix.org/

% http://www.statmethods.net/advgraphs/images/ggplotdensity.png


\section{Data Schema}
\label{sec:schema}

Using the SHA-1 hashes of the records, a local de-duplication operation is performed. The result of this operation is 249,376 unique spreadsheets, and can be downloaded from our site, or preferably, directly from S3 using the script provided on our site. The output of our pipeline also generates 719,223 JSON documents, which are intended for import into document-centric databases such as MongoDB.\footnote{\url{http://www.mongodb.org/}.} In this section, we describe the important records in this document.

The most relevant elements from the WARC record are \texttt{WARC-Target-URI}, that is, the URL from which the spreadsheet and downloaded, and \texttt{Container}, the containing CommonCrawl file and offset used to extract the spreadsheet from the crawl. The \texttt{WARC-Date} element may also be of interest, since it contains the time and date of the access. Using the \texttt{Content-Disposition} element, one can often extract the original spreadsheet file name.

The Tika JSON element contains four fields, which looks something like this:

\begin{lstlisting}
"Tika": {
    "Tika-Content-Type": 
       "application/vnd.ms-excel",
    "Tika-Extension": ".xls",
    "Digest": "sha1:...",
    "Length": 5123
}
\end{lstlisting}

These fields are self-explanatory. The \texttt{InternetDomain} element is also relatively straight-forward; we provide it because the individual domain components are useful for analysis, and because this extraction is non-trivial to perform once the data is already in a database.

\begin{lstlisting}
"InternetDomainName": {
    "Host": "www.example.org",    
    "Top-Private-Domain": "example.org",
    "WARC-Target-URI": 
      "http://www.example.org/results/test.xls",
    "Public-Suffix": "org"
  }
\end{lstlisting}

Next, we also provide a LingPipe element, which extracts the token stream from spreadsheets, lowercases the tokens, removes English stop words (such as `a' or `the'), and filters out non-words (such as numbers). Again, the representation of this is simple:

\begin{lstlisting}
"LingPipe": {    
    "Tokens": [
      "finance",
      "branch",      
      "city",
      ...
    ]
  }
\end{lstlisting}

Finally, to get a high-level overview of the content of the spreadsheets, as well as to aid other researchers in narrowing their queries, we used Apache POI  to analyze the content of the spreadsheets and provide a summary. 
There are over 450 entries, which include the number of times a given Excel function (such as \texttt{SUM} or \texttt{VLOOKUP}) is used, the total number of input cells (i.e. cells that are not formulas), the number of numeric input cells, the number of formulas used more than 50 times, the most common formula used, and so on.

% The metadata file can be downloaded from HERE, unzipped, and then loaded into mongoDB\footnote{https://www.mongodb.org/} using the following command:
% \texttt{mongoimport --db spreadsheets --collection fuse --file fuse.metadata.json}

% Standard mongoDB queries can then be done on the analysis, such as 
% \begin{lstlisting}
% db.fuse.aggregate(
%     { "$match": 
%       {"Tika.Length" : { $gt: 1000000 } } },
%     { "$group": 
%       {"_id": "Big Spreadsheets",
%  "count": {"$sum": 1} } })
% \end{lstlisting}
% which counts how many spreadsheets are larger than 1 MB.


% The metadata we collected for the indices was largely influenced by the summary statistics presented in \cite{Fisher2005}.  
% For each spreadsheet, there are over 450 entries, so we will not list them all here.
% In general, the entries summarize the contents of the cells.
% To list a few examples, the number of times a given Excel function (such as \texttt{SUM} or \texttt{VLOOKUP}) is used, the total number of input or data cells, the number of numeric input cells, the number of formulas used more than 50 times, the most common formula used, etc.


% aws s3 ls --recursive s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-10/segments | awk '$4 ~ /wat/ {print $4}' > wat.summer2013.path

% http://blogs.msdn.com/b/vsofficedeveloper/archive/2008/05/08/office-2007-open-xml-mime-types.aspx

% \section{Description of Spreadsheet Corpus}

% Using the data schema from Section~\ref{sec:schema}, and high-level numbers from the MapReduce pipeline, we can now characterize some of the interesting features in the dataset.

% \begin{table}[!t]
% \caption{Common Crawl Archive\label{tab:carchive}}
% \centering
% \begin{tabular}{lll}
% \toprule
% Description & TB & Yield\\
% \midrule
% Summer 2013 & 30.6 & 42.14\%\\
% Winter 2013 & 35.1 & 33.46\% \\
% March 2014 & 36.4 & \\
% April 2014 & 41.2 & \\
% July 2014 & 59.2 & \\
% August 2014 & 46.6 & \\
% September 2014 & 48.9 & \\
% October 2014 & 59.1 & \\
% November 2014 & 31.4 & \\
% December 2014 & 35.2 & \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{figure}[!t]
% \centering
% % \input{figures/stack.tex}
% \includegraphics[width=\columnwidth]{Rplot01}
% \caption{Cumulative count of candidate spreadsheets with each additional crawl. One problem is that unless the diversity of the crawl increases, unique spreadsheets are reaching a local maximum.}
% \label{fig:testplot}
% \end{figure}


% \subsection{Public Suffixes}

% \begin{table}[!t]
% \caption{Top 10 Public-Suffixes\label{tab:suffix}}
% \centering
% \begin{tabular}{ll}
% \toprule
% Description & Frequency\\
% \midrule
% .gov & 250,21\\
% .org & 171,845\\
% .com & 112,573\\
% .edu & 63,572\\
% .gov.au & 43,490\\
% .pa.us & 7,621\\
% .net & 7,409\\
% .mn.us & 4,641\\
% .ac.uk & 3,423\\ 
% .ca.us & 3,003 \\
% \bottomrule
% \end{tabular}
% \end{table}

% The top 10 domains from which Common Crawl spreadsheets were obtained is shown in Table~\ref{tab:suffix}. In total, we obtained 4,381 unique domains. In Common Crawl, spreadsheets are most frequently obtained from government-related sites. 143135 spreadsheets alone come from a single domain, \url{census.gov}.

% 4,381 domains. After dedup, 4,342.


% What domains are represented in the corpus?

% \begin{lstlisting}
% db.s.aggregate(
%     { "$project": 
%       {"InternetDomainName.Public-Suffix" : true } },
%     { "$group": 
%       {"_id": "$InternetDomainName.Public-Suffix", "count": {"$sum": 1} } },
%     { "$sort": { "count" : -1 } },
%     { "$limit": 10 }
% )
% \end{lstlisting}

% TLD bias: http://w3techs.com/technologies/overview/top_level_domain/all


% FOR SDL: topleveldomain

% absfreq, relfreq
% census.gov, 143135
% triathlon.org, 106486
% amamanualofstyle.com, 45118
% abs.gov.au, 42941
% utah.gov, 22242
% ohio.gov, 16739
% usda.gov, 13016
% worldbank.org, 11062
% theahl.com, 10350
% eia.gov, 8216

% \noindent\textbf{RQ: How many domains are represented?}



% TODO(tbarik): Make sure all MongoDB queries refer to the database the same way: fuse.

% how canonical are urls?

% not very

% db.s.aggregate(
%     { "$project": {"Tika.Digest": true, "WARC-Target-URI": true } },
%     { "$group": { "_id": { uri: "$WARC-Target-URI", sha: "$Tika.Digest" }, count: { "$sum": 1 } } },    
%     { "$group": { "_id": { uri: "$_id.uri" }, count: { "$sum": 1 } } },
%     { "$match": { "count" : { "$gte" : 2 } } },
%     { "$sort": { "count" : -1 } }  
% )


% euses and cc overlap
% sha1:24af0f8d6a5a18a44150a6b56587582546a0ba71  (1)
%   Inventory control form www.edu.edu
%    ./inventory/processed/InventoryControlForm.xls
% sha1:70cc5c391b4e439c61e003f0c01ea573fcea544b (3/but all same)
%     ncdenr.org
%     ./modeling/processed/labrates050104.xls
% sha1:9992cd834d4824cbb05c26b0ffcecff04509334c
%     naaccr.org
%     ./grades/processed/naaccr9a.xls
% sha1:9a09b5a28eb4faf6516ddac8aaf7c90eddc47090
%     http://www.courts.delaware.gov/forms/download.aspx?ID=21098
%     http://www.courts.delaware.gov/forms/download.aspx?id=21098
%     ./financial/processed/finanacial%20sheet.xls
% sha1:d903ce9171061b482d92aa78bb2e60404ab16732
%     nigc.gov
%     ./inventory/processed/TableInventorySlipWorksheet.xls    

% Media Type

% application/vnd.ms-excel, 238673
% application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, 10555
% application/vnd.openxmlformats-officedocument.spreadsheetml.template, 148

% all zero:
% application/vnd.ms-excel.sheet.macroEnabled.12
% application/vnd.ms-excel.template.macroEnabled.12
% application/vnd.ms-excel.addin.macroEnabled.12
% application/vnd.ms-excel.sheet.binary.macroEnabled.12

% RQ: How much can you trust HTTP headers?
% RQ: Train a text classifier to identify topicality.
% RQ: Evolution of spreadsheets


% Breakdown of analysis:
%db.spreadsheet.aggregate(
%    { "$project": 
%      {"Summary.errorNotification" : true } },
%    { "$group": 
%      {"_id": "$Summary.errorNotification", "count": {"$sum": 1} } },
%    { "$sort": { "count" : -1 } },
%    { "$limit": 10 })

% "OK", 220760
% "BIFF5",  17643 
% "OTHER", 10782 
% "CORRUPT",129
% "ENCRYPTED",  62 

%	db.fuse.aggregate(
%    { "$group": 
%      {"_id": "Input Cells", "count": {"$sum": "$totalInputCells"} } })
% Total Input cells: 357210294

%	db.fuse.aggregate(
%    { "$group": 
%      {"_id": "Total Formula Cells", "count": {"$sum": "$totalFormulas"} } })
% Total Formula cells: 10776903
% some ideas for things to show
% http://blog.commoncrawl.org/2012/05/

% Total Non-empty cells: 357210294 + 10776903 = 367987197
% Average non-empty cells per workbook: 367987197/220760 = 1667

%db.fuse.aggregate([
%	{ $match: { totalFormulas: { "$gt": 0}}},
%    { "$group": 
%      {"_id": "Workbooks with Formulas", "count": {"$sum": 1} } }])
% Number of workbooks with formulas: 14782

% Number of formulas/workbook with formulas: 10776903/ 14782 = 729

% Number of unique formulas: 894361

% Number of unique formulas/workbook with formulas: 894361/ 14782 = 60

% Number of different functions used: 219

%db.fuse.aggregate(
%    { "$group": 
%      {"_id": "Total Sheets", "count": {"$sum": "$numSheets"} } })
% Total Sheets: 346247

%db.fuse.aggregate(
%	{ "$group": 
%      {"_id": "Maximum Sheets", "count": {"$max": "$numSheets"} } })
% Maximum number of sheets: 147

%>  db.fuse.aggregate({ "$group":{"_id": "Total countAVERAGEIF Cells", "count": {"$sum": "$countAVERAGEIF"} } })
%{ "_id" : "Total countAVERAGEIF Cells", "count" : 775 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countAVERAGEIFS Cells", "count": {"$sum": "$countAVERAGEIFS"} } })
%{ "_id" : "Total countAVERAGEIFS Cells", "count" : 162 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countCOUNTIF Cells", "count": {"$sum": "$countCOUNTIF"} } })
%{ "_id" : "Total countCOUNTIF Cells", "count" : 51239 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countCOUNTIFS Cells", "count": {"$sum": "$countCOUNTIFS"} } })
%{ "_id" : "Total countCOUNTIFS Cells", "count" : 1249 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countIF Cells", "count": {"$sum": "$countIF"} } })
%{ "_id" : "Total countIF Cells", "count" : 2496350 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countIFERROR Cells", "count": {"$sum": "$countIFERROR"} } })
%{ "_id" : "Total countIFERROR Cells", "count" : 181976 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countIFNA Cells", "count": {"$sum": "$countIFNA"} } })
%{ "_id" : "Total countIFNA Cells", "count" : 0 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countSUMIF Cells", "count": {"$sum": "$countSUMIF"} } })
%{ "_id" : "Total countSUMIF Cells", "count" : 94978 }
%>  db.fuse.aggregate({ "$group":{"_id": "Total countSUMIFS Cells", "count": {"$sum": "$countSUMIFS"} } })
%{ "_id" : "Total countSUMIFS Cells", "count" : 239 }

% \section{Kevin's Crap}

% Programming in spreadsheets:

% 154596 of our unique formulas contained \texttt{IF}, or one of its cousins like SUMIF, COUNTIF. 
% %grep -E "[^a-zA-Z]IF\\(" .\fuse-allPlusCounts.txt > just-if.txt
% 150461 of our unique formulas contained IF (and maybe other functions).
% %grep -E "[^a-zA-Z]IF.*[^a-zA-Z]IF" .\fuse-allPlusCounts.txt > just-if-squared.txt
% 44584 of our unique formulas contained IF two or more times.
% 9816 unique formulas used IF 5 or more times, typically in a nested fashion
% 302 unique formulas used IF ten or more times, typically in a nested fashion.  
% This may indicate a need for a more robust branching.

% 2496350 of our 10 million formula cells used an IF function, 94978 cells contained a SUMIF (these numbers may overlap as a cell may have both a SUMIF and an IF (we noted over 5000 unique formulas that had both an IF and a SUMIF)

    
% The function breakdown between the three corpora is interesting.  Some functions are used a similar amount (e.g. IF) and others are used differently (e.g. SUM, ISBLANK, VLOOKUP, ISTEXT).
% This underscores the need for large, diverse corpora. 
% For example, all three corpora used IF about the same, whereas \textsc{Fuse} contains many more string-manipulation functions and Enron uses more financial functions.  
% It may also be interesting to explore function ``synonyms'', which occur when there is more than one way to achieve the same result.  
% For example, in \textsc{Fuse} and Enron, workbooks are more likely to use the + operator than the SUM function, but in EUSES, those tools are used at about the same rate.

\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\centering
\label{tab:selectedFunctions}

\caption{Selected Functions with counts per 1000 formula cells}
\begin{tabular}{lllll}
\toprule
 & \textbf{\textsc{Fuse}} & \textbf{Enron} & \textbf{EUSES}\\
\midrule
IF & 178.8 & 156.9 & 166.8\\
+ (operator) & 166.4 & 217.5 & 167.6\\
SUM & 87.7 & 80.3 & 153.6\\
ISBLANK & 57.8 & 0.1 & 27.4\\
VLOOKUP & 30.8 & 52.3 & 12.2\\
HLOOKUP & 9.5 & 2.8 & 1.4\\
AVERAGE & 32.0 & 9.2 & 7.9\\
AND & 6.6 & 15.9 & 21.7\\
NOT & 5.5 & 0.0 & 1.9\\
\bottomrule

\end{tabular}

\end{table}


% \begin{table}[!t]
% \caption{Spreadsheet Classification\label{tab:ccrawl}}
% \centering
% \begin{tabular}{lll}
% \toprule
% \textbf{Category} & \textbf{\textsc{Fuse}} & \textbf{EUSES}\\
% \midrule
% Database & 4518 & 720\\
% Finance & 3058 & 780\\
% Grade & 2915 & 731\\
% Homework & 61 & 682\\
% Inventory & 2243 & 756\\
% Model & 2143 & 966\\
% \bottomrule
% \end{tabular}
% \end{table}

% The results in this section are intended to demonstrate the essential properties of the corpus.

% \subsection{RQ2: How diverse is the common common crawl corpus?}
% \subsection{RQ3: NLP Extraction of Spreadsheet?}
% \subsection{RQ4: What types of formulas ar eused by end-user software programmers?}

% Top two formulas are the same across the three corpora: 
% 1. Add up the three cells to my left
% 2. Add up the two cells to my left
% And most of the top 10 are Add up n cells to my left.
% Fuse \#3 HYPERLINK("http://www.eia.doe.gov/totalenergy/data/monthly/dataunits.cfm","Note: Information about data precision and revisions.")
% Enron \#3 NOW()  

% stage 1 metadata

\section{Challenges and Limitations}

One of the limitations of the \textsc{Fuse} corpus is that in being derived from Common Crawl, the corpus is also constrained by what the Common Crawl considers to be relevant. Since the Common Crawl is a general crawling infrastructure, spreadsheet content arises infrequently. Still, we obtained 249,376 unique spreadsheets from this extraction, a non-trivial numbers. A second limitation of our crawl is that we cannot infer the quality of the spreadsheets. In contrast with Euses, whose extraction was guided by page ranking, no such ranking function is exposed in Common Crawl.

Yet another challenge is that it is impossible to provide a set of metadata that is satisfactory to all researchers, since the potential features of interest within a binary spreadsheet are innumerable. It is expected that researchers will have to integrate into our mixin stage to obtain specific metrics of interest. However, this process is considerably simpler and less expensive than having to do obtain the spreadsheets from the original Common Crawl data.

% https://gist.github.com/anonymous/8373f8a08a357146c20b

% https://groups.google.com/forum/#!topic/common-crawl/MV5yYWPWC_M
% As Kevin said, you might be able to get away using off the shelf search APIs, depending on exactly how much you want. One thing I will note though is that a MapReduce job over the relevant text data of Common Crawl won't cost hundreds of dollars, it would more likely cost somewhere between $30 and $60, possibly far cheaper if you run particularly optimized code.

% To combine all of them explicitly would just result in duplication of stored files, which is quite an issue when we're talking hundreds of terabytes.

% As all the files are stored in ARC or WARC, both web archive formats, the easiest way to combine them for your "full crawl" is to simply enumerate over all the files in all the crawls. This also allows you to decide what behaviour you would like (i.e. keep all pages, keep only the most recent pages, etc).

% https://groups.google.com/forum/#!topic/common-crawl/wb3jXh8x8Tg
% There are far more than 4.05 billion new web pages in the last three months. These crawls shouldn't be considered representative of how much of the web has changed or been updated in a given period of time as we only crawl a relatively small portion of the web. It also strongly depends on your definition of new, though that's a large and complicated situation all to itself.

% relevance is dependent on common crawl definition of relevant

\section{Conclusion}

This paper contributes a spreadsheet corpus, \textsc{Fuse}, which is derived from the Common Crawl. In comparison with other corpora, \textsc{Fuse} stacks up well. \textsc{Fuse} is a relatively balanced dataset that contains a significant number of spreadsheets, can be queried because of its associated JSON metadata documents, and is easily reproducible and extendable because because it derives from a source that couples both the metadata with the raw crawl data. It is also more up-to-date than existing corpora, and can remain up-to-date as additional crawls can be incorporated into \textsc{Fuse}.

An additional property that separates \textsc{Fuse} from existing corpora is that is both a dataset and an open source tool framework. Consequently, our code can not only be inspected for correctness (and defects resolved when issues are identifies), but our programs can also be modified and tailored to support specific research needs, which we have exposed through mixins.

In many cases, we expect that \textsc{Fuse} can replace existing corpora, with stronger guarantees concerning reproducibility. In other cases, we expect that \textsc{Fuse} can offer an excellent complement existing corpora in order to increase the diversity and representativeness of tool evaluations for end-user programmers.

% \section{Related Work}
% % http://link.springer.com/article/10.1007/s10664-011-9181-9
% \subsection{Why use spreadsheets}
% \cite{Chambers2010} Use spreadsheet corpus + interviews to determine which features end-users use.
% ~\cite{Pinzger2012} Detecting code smells in spreadsheets. Analyze EUSES to study occurrence of smells.
% ~\cite{Badame2012} Refactor spreadsheet formula. Perform case study using EUSES dataset.
% ~\cite{Abraham2007} Support debugging spreadsheets

% \subsection{What other corpora?}
% EUSES~\cite{Fisher2005}

% ~\cite{Chen2013} Automatically extract relational data from spreadsheets. Extracted 410,554 spreadsheets from clue09 web crawl.

% ~\cite{Hermans2015} ENRON find citation <-- icse seip 2015

% ~\cite{Chen2013} Automatically extract relational data from spreadsheets. Extracted 410,554 spreadsheets from clue09 web crawl.

% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}

This material is based upon work supported in whole or in part with funding from the Laboratory for Analytic Sciences (LAS). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the LAS and/or any agency or entity of the United States Government.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

\raggedright
% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{library}


% that's all folks
\end{document}


